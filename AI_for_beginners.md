
# 人工智能基础

### 从生物学开始
  大脑中的细胞叫神经元，每个神经元都可以和上千个神经元建立连接，神经元是大脑处理信息的基本单元。
  
### 神经元

神经元有一个细胞体，向周围不同的延伸，大部分是树枝状的树枝。有一个比较长的延伸（也可能是分支）称为轴突。虚线显示轴突丘，信号开始传输。
![brainneuron](https://www.codeproject.com/KB/AI/NeuralNetwork_1/brainneuron.png)

神经元的边界被称为细胞膜。膜内外有电压差（膜电位）。

如果输入足够大，就会产生动作电位。动作电位（神经元轴突）然后沿着轴突向下移动，远离细胞体。
![brainneuronspike](https://www.codeproject.com/KB/AI/NeuralNetwork_1/brainneuronspike.png)

### 突触(Synapses)
一个神经元与另一个神经元之间的连接称为突触。信息总是通过轴突传递，然后通过突触传递到接收神经元。

### 神经元放电
神经元只在输入大于某个阈值时才激发。然而，应该注意到，随着刺激的增加，动作电位不会变得更大，即有或无两种状态。
![brainneuronfiring](https://www.codeproject.com/KB/AI/NeuralNetwork_1/brainneuronfiring.png)

尖峰（信号）很重要，因为其他神经元接收它们。神经元之间用尖峰信号相交流。发送的信息是用尖峰信号编码的。

### 对神经元的输入
突触可以处于兴奋性状态，也可以是抑制性状态。

到达兴奋性突触的尖峰信号会导致接收神经元激发。到达抑制性突触的尖峰（信号）趋向于抑制接收神经元的放电。

细胞体和突触本质上是计算（由复杂的化学/电过程）传入的兴奋性和抑制性输入之间的差异（空间和时间的总和）。

当这个差异足够大（与神经元的阈值相比），神经元就会激发。粗略地说，更快的兴奋性尖峰到达它的突触，它会更快地激发（类似于抑制性尖峰）。

### 人工神经网络
假设每个神经元都有一个激发概率，同时假设一个神经元与其他m个神经元相连，所以这个神经元能接收到m个信号输入 "x1 …. … xm"，看起来像这样：

![nn1](https://www.codeproject.com/KB/AI/NeuralNetwork_1/nn1.png)

这个模型叫感知器，是最早的神经网络模型，感知器模型通过输入带有加权数的输入来模拟神经元，如果加权的和大于某个可调的阈值就输出1，否则就输出0，也叫
激活函数（activation function）。

图中的输入（X1，X2，X3，XM）和连接权值（W1，W2，W3，WM）通常是实际值，都有正（+）和负（-），如果某些Xi的特征倾向于使感知器激发，则Wi的权重将是正的；如果特征Xi抑制感知器，则Wi权重为负。

感知器本身由权重、求和处理器和激活函数, 还有一个可调阈值处理器（后称为偏好）组成。为了方便起见，通常的做法是将偏好视为另一种输入。下图说明了修改后的配置：
![nn2](https://www.codeproject.com/KB/AI/NeuralNetwork_1/nn2.png)
这个偏好可以认为是感知器激发的倾向（倾向于特定行为方式）不管输入是什么，如上图显示的，如果权重之和大于0那么感知器模型激发，用数学模型解释就是：
![summer](https://www.codeproject.com/KB/AI/NeuralNetwork_1/summer.png)

### 激活功能
激活通常使用以下函数中的一种。
#### Sigmoid函数
输入越强，神经元激发速度越快（更高的激发率），Sigmoid函数在多层网络中也很有用，因为Sigmoid函数曲线允许分化（在多层网络的反向传播训练需要这样）。
![sigmoid](https://www.codeproject.com/KB/AI/NeuralNetwork_1/sigmoid.png)
数学模型表达：
![sigmoidmaths](https://www.codeproject.com/KB/AI/NeuralNetwork_1/sigmoidmaths.png)

#### 阶跃函数
一个基本的开关函数，if 0 > x then 0, else if x >= 0 then 1
![step](https://www.codeproject.com/KB/AI/NeuralNetwork_1/step.png)
数学模型：
![stepmaths](https://www.codeproject.com/KB/AI/NeuralNetwork_1/stepmaths.png)

### 学习
在我们继续讨论感知器学习之前，让我们考虑一个真实世界的例子：

你怎样教孩子认识椅子？你给他看一个东西，告诉他：“这是椅子。那些不是椅子“，直到孩子学会椅子的概念。在这个阶段，孩子可以看我们展示给他的东西，当他问：“这个物体是椅子吗？”，我们给他正确的答案。

此外，如果我们向孩子展示他以前从未见过的新物体，我们可以期望他正确地认识到新物体是否是椅子，只要我们给了他足够的正面和反面的例子。

这就是感知器模型背后的核心思想。

### 感知器中的学习
感知器的学习就是不断调整权重和偏好的过程。
感知器是一个学习概念的程序，也就是说，它可以学会用true（1）或false（0）来响应我们给它的输入，通过反复地学习呈现给它的例子。

感知器是一个单层神经网络，当输入相应的输入向量时，它的权值和偏差可以被训练成产生正确的目标向量。所使用的训练技术称为感知器学习规则。

### 学习规则
感知器对应每个输入向量都有一个输出为0或1的目标输出向量，如果解存在，那么学习规则则被证明在有限的时间内收敛于一个解。

学习规则可以归纳为以下两个方程：

b = b + [ T - A ]

对于所有输出的 i:

W(i) = W(i) + [ T - A ] * P(i)

其中W是权值的向量，P是呈现给网络的输入向量，T是神经元应该显示的正确结果，A是神经元的实际输出，b是偏差。

### 训练
训练集中的向量一个接一个地呈现在网络上。如果网络输出正确，则不作任何更改，因为W(i) = W(i) + [ T - A ] * P(i)，当T=A，即实际输出是正确的结果，
输入不在影响输出。如果输出不正确，则使用感知器学习规则更新权重和偏差（如上所示）。当训练集的每一个纪元（遍历完一次整个输入训练向量被称为一个纪元）都没有发生错误时，则训练就完成了。

此时，任何输入训练向量都可以呈现给网络，并且网络可以用正确的输出向量来响应输入。

如果不是在训练集里的一个向量P被呈现给网络，网络会响应一个和目标向量近似的输出。

### 那么我们可以用神经网络做什么呢？
#### 单层神经网络
单层神经网络（感知网络）是输出单元独立于其他网络的网络——每一个权值只影响一个输出。用它可以实现线性seperability函数，如下图（假设这个网络有2个输入和1个输出）

![linear](https://www.codeproject.com/KB/AI/NeuralNetwork_1/linear.png)

可以看出，这相当于AND和OR逻辑门，如下所示:

![logicgates](https://www.codeproject.com/KB/AI/NeuralNetwork_1/logicgates.png)

然而感知器网络有其局限性。如果向量不是线性可分的，那么学习就永远达不到所有向量被正确分类的地步。感知器无力解决线性不可分的载体问题，最著名的例子是布尔XOR问题。

#### 多层神经网络
多层神经网络可以解决线性不可分的问题，例如上面提到的XOR问题，下一篇文章则会介绍多层神经网络。
